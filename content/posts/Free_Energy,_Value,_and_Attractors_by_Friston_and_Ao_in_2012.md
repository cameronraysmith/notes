+++
title = "Free Energy, Value, and Attractors by Friston and Ao in 2012"
author = ["Cameron Smith"]
lastmod = 2020-09-20T17:34:29-04:00
slug = "Free_Energy,_Value,_and_Attractors_by_Friston_and_Ao_in_2012"
draft = false
+++

## 2. Ensemble dynamics and random attractors {#2-dot-ensemble-dynamics-and-random-attractors}


### 2.1 Set up: states and dependencies {#2-dot-1-set-up-states-and-dependencies}


### 2.2 Dynamics and ergodicity {#2-dot-2-dynamics-and-ergodicity}


### 2.3 Global random attractors {#2-dot-3-global-random-attractors}


### 2.4 Autopoiesis and attracting sets {#2-dot-4-autopoiesis-and-attracting-sets}


### 2.5 Summary {#2-dot-5-summary}


## 3. The free energy formulation {#3-dot-the-free-energy-formulation}


### Introduction {#introduction}

<!--list-separator-->

-  the free energy principle states that the conditional entropy of an agent's states is minimized through action

<!--list-separator-->

-  \\[a^\* = \mathrm{arg\,min}\_a \mathscr{H} (X | m) = \mathrm{arg\,min}\_a \mathscr{H} (S | m)\\]

<!--list-separator-->

-


### 3.1 Active inference and generalized filtering {#3-dot-1-active-inference-and-generalized-filtering}


### 3.2 Summary {#3-dot-2-summary}


## 4. Policies and value {#4-dot-policies-and-value}


### 4.1 Conservative (Divergence-Free) flow {#4-dot-1-conservative--divergence-free--flow}


### 4.2 Dissipative (Curl-Free) flow and detailed balance {#4-dot-2-dissipative--curl-free--flow-and-detailed-balance}


### 4.3 Summary {#4-dot-3-summary}


## 5. Optimal (fixed point) control and reinforcement-learning {#5-dot-optimal--fixed-point--control-and-reinforcement-learning}


### 5.1 Optimal control theory {#5-dot-1-optimal-control-theory}


### 5.2 Summary {#5-dot-2-summary}


## 6. Generalized (itinerant) policies {#6-dot-generalized--itinerant--policies}


### 6.1 The mountain car problem {#6-dot-1-the-mountain-car-problem}


### 6.2 Optimal itinerancy and weakly attracting sets {#6-dot-2-optimal-itinerancy-and-weakly-attracting-sets}


### 6.3 Summary {#6-dot-3-summary}


## 7. Discussion {#7-dot-discussion}


### 7.1 Dynamics versus reinforcement-learning {#7-dot-1-dynamics-versus-reinforcement-learning}


### 7.2 Value-learning versus perceptual learning {#7-dot-2-value-learning-versus-perceptual-learning}


## 8. Conclusion {#8-dot-conclusion}


## Appendices {#appendices}


### A. Entropy production {#a-dot-entropy-production}


### B. Sensory entropy {#b-dot-sensory-entropy}


### C. The Laplace assumption {#c-dot-the-laplace-assumption}


### D. Value and ergodic densities {#d-dot-value-and-ergodic-densities}


### E. Cost functions and value {#e-dot-cost-functions-and-value}


### F. Optimal control and policies {#f-dot-optimal-control-and-policies}


### G. Value learning and optimality equations {#g-dot-value-learning-and-optimality-equations}


### H. Integrating active inference schemes {#h-dot-integrating-active-inference-schemes}
